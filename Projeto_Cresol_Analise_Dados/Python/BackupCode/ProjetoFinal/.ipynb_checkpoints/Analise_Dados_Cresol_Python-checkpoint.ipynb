{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Análise de Dados - Desafio Cresol Confederação\n",
    "\n",
    "# Desenvolvedor: Bruno Scovoli Bruneli\n",
    "# Data: 26/05/2019\n",
    "# Email: brunobrunelli.bb@gmail.com\n",
    "# LinkedIn: https://www.linkedin.com/in/brunobruneli/\n",
    "\n",
    "# Descrição do Projeto:\n",
    "# Este código tem como objetivo a extração, limpesa, transformação, \n",
    "# análise e carregamento de todo conteúdo extraído de um arquivo de \n",
    "# log de serviços de hospedagem proposto pelo teste de conhecimento \n",
    "# da Cresol Confederação.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação dos Pacotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração de dados a partir de um arquivo \".log\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_dados_arquivo(nome_arquivo):\n",
    "    # Lógica de leitura dos dados do arquivo de Logs:\n",
    "    with open(nome_arquivo,mode=\"r\",encoding=\"utf-8\",newline=\"\\r\\n\") as f:\n",
    "        result = f.readlines()\n",
    "        \n",
    "    # Remove linhas em branco:\n",
    "    for i in result:\n",
    "        if i == '\\r\\n':\n",
    "            result.remove('\\r\\n')\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tramento dos dados de entrada extraídos do arquivo de log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratar_dados(result):\n",
    "    listaLog = []\n",
    "    \n",
    "    for i in range(len(result)):\n",
    "        lista, user_agent_lista= [], []\n",
    "\n",
    "        try:\n",
    "            # Separando o conteudo dentro de cada item da lista:\n",
    "            aux = shlex.split(result[i], posix=False)\n",
    "\n",
    "            # Lógica para percorrer as 9 primeiras colunas, inserindo na lista:\n",
    "            for f in range(10):\n",
    "                # Removendo caracter:\n",
    "                aux[f] = aux[f].replace('\"','')\n",
    "\n",
    "                # Lógica para remover a coluna '-':\n",
    "                if f != 1 and f != 3:\n",
    "                    \n",
    "                    # Lógica para incluir duas novas colunas (DATE e TIME):\n",
    "                    if f == 4: \n",
    "                        aux[f] = aux[f].replace('[','').replace(']','')\n",
    "                        lista.append(datetime.strptime(aux[f],'%Y-%m-%dT%H:%M:%SZ').strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                        \n",
    "                    elif f == 5:\n",
    "                        # Pegamos apenas primeiro conteúdo da lista criada a partir 'aux' após separarmos:\n",
    "                        lista.append(aux[f].split(' ')[0])\n",
    "\n",
    "                        # Pegamos somente o valor de endpoint:\n",
    "                        lista.append('/'.join(aux[5].split(' ')[1].split('/')[0:2]))\n",
    "                        \n",
    "                    elif f == 9:\n",
    "                        # Remove caracteres indesejados da coluna:\n",
    "                        aux[f] = aux[f].replace('\\r\\n','')\n",
    "                        lista.append(aux[f].split(' ')[0])\n",
    "                        lista.append(aux[f])\n",
    "                        \n",
    "                    else:\n",
    "                        lista.append(aux[f])\n",
    "\n",
    "            # Incluímos os valores tratados da lista em uma sublista:\n",
    "            listaLog.append(lista)\n",
    "            \n",
    "            # Excluímos as listas carregadas da memória para desalocar espaço:\n",
    "            del lista\n",
    "            del user_agent_lista\n",
    "\n",
    "        except:\n",
    "            print('Erro: A lista contém valores que não são esperados na linha - '+ str(i))\n",
    "    \n",
    "    return listaLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisa_dados(lista_resultado):\n",
    "    # Criação de um DataSet a partir dos dados extraídos do arquivo:\n",
    "    \n",
    "    lista_result_log = lista_resultado\n",
    "    \n",
    "    # Definindo uma lista com as Colunas do DataFrame:\n",
    "    labels = ['IP_ADDRESS','USER','DATETIME','METHOD','ENDPOINT','STATUS_CODE','SIZE_OBJECT','REFER','BROWSER','USER_AGENT']\n",
    "    \n",
    "    # Criação do DataFrame a partir da lista de Logs:\n",
    "    dfLog = pd.DataFrame(lista_result_log,columns=labels)\n",
    "    \n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    # os 5 (cinco) logins que mais efetuaram requisições:\n",
    "    top5_user = dfLog['USER'].sort_values(ascending=False).value_counts().head(5)\n",
    "    top5_user = list(dict(top5_user).keys())\n",
    "    \n",
    "    # os 10 (dez) browsers mais utilizados:\n",
    "    top10_browser = dfLog['BROWSER'].value_counts().head(10)\n",
    "    top10_browser = list(dict(top10_browser).keys())\n",
    "    \n",
    "    # os endereços de rede (classe C) com maior quantidade de requisições:\n",
    "    top10_end_req = dfLog['IP_ADDRESS'][(pd.to_numeric(dfLog['IP_ADDRESS'].str.split('.').str[0]) >= 192) & \\\n",
    "                                    (pd.to_numeric(dfLog['IP_ADDRESS'].str.split('.').str[0]) <= 223)].value_counts().head(10)\n",
    "\n",
    "    top10_end_req = list(dict(top10_end_req).keys())\n",
    "    \n",
    "    # Selecionando somente as colunas que serão utilizadas:\n",
    "    dfbymin = dfLog.iloc[:,1:8]\n",
    "    \n",
    "    # Convertendo as colunas para os formatos desejaveis:\n",
    "    dfbymin[\"HORA\"] = pd.to_datetime(dfbymin.loc[:,\"DATETIME\"]).dt.hour \n",
    "    dfbymin[\"MINUTE\"] = pd.to_datetime(dfbymin.loc[:,\"DATETIME\"]).dt.minute\n",
    "    dfbymin[\"SIZE_OBJECT\"] = pd.to_numeric(dfLog[\"SIZE_OBJECT\"])\n",
    "    dfbymin[\"STATUS_CODE\"] = pd.to_numeric(dfLog[\"STATUS_CODE\"])\n",
    "    \n",
    "    # a hora com mais acesso no dia:\n",
    "    hora_sucesso = dfbymin.iloc[:,[7]][\"HORA\"].value_counts().idxmax()\n",
    "    \n",
    "    # a hora com a maior quantidade de bytes:\n",
    "    maior_hora_byte = dfbymin.groupby(\"HORA\").agg(\"SIZE_OBJECT\").mean().sort_values(ascending=False).idxmax()\n",
    "    \n",
    "    # o endpoint com maior consumo de bytes:\n",
    "    endpoint_maior_byte = dfbymin.groupby(\"ENDPOINT\").agg(\"SIZE_OBJECT\").mean().sort_values(ascending=False).idxmax()\n",
    "    \n",
    "    # a quantidade de bytes por minuto:\n",
    "    qtd_byte_min = dict(dfbymin.groupby(\"MINUTE\").agg(\"SIZE_OBJECT\").mean())\n",
    "    \n",
    "    # a quantidade de bytes por hora:\n",
    "    qtd_byte_hora = dict(dfbymin.groupby('HORA').agg('SIZE_OBJECT').mean())\n",
    "    \n",
    "    # a quantidade de usuários por minuto:\n",
    "    dfqtd_min = dfbymin.iloc[:,[0,8]].drop_duplicates()\n",
    "    qtd_user_min = dict(dfqtd_min.groupby([\"MINUTE\"]).size())\n",
    "    \n",
    "    # a quantidade de usuários por hora:\n",
    "    dfqtd_hora = dfbymin.iloc[:,[0,7]].drop_duplicates()\n",
    "    qtd_user_hora = dict(dfqtd_hora.groupby([\"HORA\"]).size())\n",
    "    \n",
    "    # a quantidade de requisições que tiveram erro de cliente, agrupadas por erro:\n",
    "    df_error = dfbymin.iloc[:,[0,4]][(dfbymin[\"STATUS_CODE\"]>= 400) & (dfbymin[\"STATUS_CODE\"]<= 499)]\\\n",
    "    .groupby([\"STATUS_CODE\"]).size().sort_values(ascending=False)\n",
    "\n",
    "    req_cliente = dict(df_error)\n",
    "    \n",
    "    # a quantidade de requisições que tiveram sucesso:\n",
    "    qtd_sucesso = list(dfbymin.iloc[:,[4]][(dfbymin[\"STATUS_CODE\"]>= 200) & (dfbymin[\"STATUS_CODE\"]<= 226)].count().values)\n",
    "    \n",
    "    # a quantidade de requisições que foram redirecionadas:\n",
    "    qtd_redirecionado = list(dfbymin.iloc[:,[4]][(dfbymin[\"STATUS_CODE\"]>= 300) & (dfbymin[\"STATUS_CODE\"]<= 308)].count())\n",
    "    \n",
    "    # Criação de uma lista com os resultados das análises:\n",
    "    lista_resultado = [top5_user,top10_browser,top10_end_req,hora_sucesso,maior_hora_byte,endpoint_maior_byte,\\\n",
    "                   qtd_byte_min,qtd_user_min,qtd_user_hora,req_cliente,qtd_sucesso,qtd_redirecionado]\n",
    "    \n",
    "    return lista_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grava saída de dados em um arquivo CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grava_csv(nome_arquivo,lista_valores):\n",
    "    # Retornamos a data atual para gravar no nome do arquivo:\n",
    "    current_date = str(datetime.now().date())\n",
    "    \n",
    "    # Realizamos a gravação dos dados extraídos da análise do log:\n",
    "    with open(str(nome_arquivo)+'_'+current_date+'.csv','w', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=';') \n",
    "        writer.writerows(map(lambda x: [x], lista_valores))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada das funções listadas acima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza a leitura do arquivo:\n",
    "result_data = extrair_dados_arquivo(\"apacheLogTest.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza a limpeza dos dados extraídos:\n",
    "lista_result_log = tratar_dados(result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza a análise dos dados:\n",
    "lista_analise_log = analisa_dados(lista_result_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza a gravação dos dados:\n",
    "grava_csv(\"apacheLogTest_analisado_by_BrunoBruneli\",lista_analise_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

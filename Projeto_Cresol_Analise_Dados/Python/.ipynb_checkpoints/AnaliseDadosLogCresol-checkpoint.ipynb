{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Dados - Desafio Cresol Confederação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Desenvolvedor: Bruno Scovoli Bruneli\n",
    "- Data: 26/05/2019\n",
    "- Email: brunobrunelli.bb@gmail.com\n",
    "- LinkedIn: https://www.linkedin.com/in/brunobruneli/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Descrição do Projeto:\n",
    "Este código tem como objetivo a extração, limpesa, transformação, análise e carregamento de todo conteúdo extraído de um arquivo de log de serviços de hospedagem proposto pelo teste de conhecimento da Cresol Confederação.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração de dados do arquivo de log.gz:\n",
    "def extrair_dados_arquivo():\n",
    "    # Criação de um dataframe a partir da leitura de dados de um arquivo.log:\n",
    "    df_data = pd.read_csv('apache.log.gz',compression = 'gzip',sep = \" \",header = None,\\\n",
    "        names = ['ip_address', 'ifen', 'user', 'user_code', 'timestamp','request', 'html_code',\\\n",
    "         'size_object', 'url_refer', 'browser']\n",
    "        ).drop(columns=['ifen'])\n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reestruturação e modificação das colunas do dataframe:\n",
    "def modifica_dados_dataframe(df_data):\n",
    "    \n",
    "    # Formatação da coluna de data e hora:\n",
    "    df_data.timestamp = pd.to_datetime(df_data.timestamp, format='[%Y-%m-%dT%H:%M:%SZ]')\n",
    "    \n",
    "    # Definindo a coluna de data e hora como indice do dataframe:\n",
    "    df_data.set_index(df_data.timestamp, inplace=True)\n",
    "\n",
    "    # Extraindo  \n",
    "    df_data.browser = df_data.browser.apply(lambda x: x.split(\" \")[0])\n",
    "    \n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extração e análise dos dados do arquivo de log.gz:\n",
    "def analisar_dados_dataframe(df_data):\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Realizamos um Groupby do dataframe pela coluna \"USER\" retornando em formato de lista:\n",
    "    top_logins = df_data.groupby(by='user').size().sort_values(ascending=False).head().reset_index().user\n",
    "    top_logins = list(top_logins)\n",
    "    \n",
    "    # Realizamos um Groupby do dataframe pela coluna \"BROWSER\" retornando os 10 primeiros registros em formato de lista:\n",
    "    top_browsers = df_data.groupby(by='browser').size().sort_values(ascending=False).head(10).reset_index().browser\n",
    "    top_browsers = list(top_browsers)\n",
    "\n",
    "    # Realizamos um Groupby do dataframe pela coluna \"IP_ADDRESS\" retornando os ips de classe \"C\" em formato de lista:\n",
    "    top_endereco_rede = df_data.loc[df_data.ip_address.apply(lambda x: int(x.split('.')[0]) >= 192 and int(x.split('.')[0]) <= 224)]\\\n",
    "    .groupby(by='ip_address').size().sort_values(ascending=False).reset_index().ip_address\n",
    "    top_endereco_rede = list(top_endereco_rede)\n",
    "\n",
    "    # Realizamos um resample do dataframe pela \"HORA\" retornando a hora de mais acesso no dia:\n",
    "    hora_acesso_dia = df_data.resample('H').size().sort_values(ascending=False).index[0].hour\n",
    "\n",
    "    # Realizamos um resample do dataframe pela \"HORA\" retornando a hora de maior tamanho em bytes:\n",
    "    hora_bytes = df_data.size_object.resample('H').sum().sort_values(ascending=False).index[0].hour\n",
    "    \n",
    "    # Realizamos um Groupby do dataframe pela coluna \"ENDPOINT\" retornando o endpoint de maior tamanho:\n",
    "    maior_endpoint = df_data.assign(endpoint = df_data.request.apply(lambda x: x.split(\"/\")[1]))\\\n",
    "    .groupby('endpoint').size().sort_values(ascending=False).index[0]\n",
    "    \n",
    "    # Realizamos um resample do dataframe pelo \"MINUTO\" retornando a quantidade de minutos de maior tamanho:\n",
    "    qtd_bytes_min = df_data.size_object.resample('Min').sum().mean()\n",
    "    \n",
    "    # Realizamos um resample do dataframe pelo \"HORA\" retornando a quantidade de horas de maior tamanho:\n",
    "    qtd_bytes_hora = df_data.size_object.resample('H').sum().mean()\n",
    "\n",
    "    # Realizamos um resample do dataframe pelo \"MINUTO\" retornando a quantidade de minutos por usuário:\n",
    "    qtd_user_min = df_data.user.resample('Min').size().mean()\n",
    "\n",
    "    # Realizamos um resample do dataframe pelo \"HORA\" retornando a quantidade de horas por usuário:\n",
    "    qtd_user_hora = df_data.user.resample('H').size().mean()\n",
    "\n",
    "    # Realizamos um Groupby do dataframe pela coluna \"HTML_CODE\" retornando dicionario com \"erro de cliente\":\n",
    "    qtd_req_by_cliente = df_data.loc[df_data.html_code.apply(lambda x: int(x)>= 400 and int(x) <= 499)]\\\n",
    "    .groupby('html_code').size().sort_values(ascending=False).to_dict()\n",
    "\n",
    "    # Realizamos um Groupby do dataframe pela coluna \"HTML_CODE\" retornando dicionario com \"sucessos\":\n",
    "    qtd_req_sucesso = df_data.loc[df_data.html_code.apply(lambda x: int(x) >= 200 and int(x) <= 226 )].shape[0]\n",
    "\n",
    "    # Realizamos um Groupby do dataframe pela coluna \"HTML_CODE\" retornando dicionario com \"redirecionadas\":\n",
    "    qtd_req_redirecionada = df_data.loc[df_data.html_code.apply(lambda x: int(x)>= 300 and int(x) <= 308)].shape[0]\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Lista de dados esperado como saída:\n",
    "    # os 5 (cinco) logins que mais efetuaram requisições;\n",
    "    # os 10 (dez) browsers mais utilizados;\n",
    "    # os endereços de rede (classe C) com maior quantidade de requisições;\n",
    "    # a hora com mais acesso no dia;\n",
    "    # a hora com a maior quantidade de bytes;\n",
    "    # o endpoint com maior consumo de bytes;\n",
    "    # a quantidade de bytes por minuto;\n",
    "    # a quantidade de bytes por hora;\n",
    "    # a quantidade de usuários por minuto;\n",
    "    # a quantidade de usuários por hora;\n",
    "    # a quantidade de requisições que tiveram erro de cliente, agrupadas por erro;\n",
    "    # a quantidade de requisições que tiveram sucesso;\n",
    "    # a quantidade de requisições que foram redirecionadas;\n",
    "    \n",
    "    \n",
    "    df_output = pd.DataFrame(\n",
    "        {\n",
    "        'top_logins': [top_logins],\n",
    "        'top_browsers': [top_browsers],\n",
    "        'top_endereco_rede': [top_endereco_rede],\n",
    "        'hora_acesso_dia': [hora_acesso_dia],\n",
    "        'hora_bytes': [hora_bytes],\n",
    "        'maior_endpoint': [maior_endpoint],\n",
    "        'qtd_bytes_min': [qtd_bytes_min],\n",
    "        'qtd_bytes_hora': [qtd_bytes_hora],\n",
    "        'qtd_user_min': [qtd_user_min],\n",
    "        'qtd_user_hora': [qtd_user_hora],\n",
    "        'qtd_req_by_cliente': [qtd_req_by_cliente],\n",
    "        'qtd_req_sucesso': [qtd_req_sucesso],\n",
    "        'qtd_req_redirecionada': [qtd_req_redirecionada]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que realiza a gravação dos dados em um arquivo .csv:\n",
    "def grava_dados_csv(lista_dados):\n",
    "    lista_dados.to_csv('resultado_analise_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processo Iniciado - 2019-05-27 14:36:18.923310 ...\n",
      "\n",
      "Processo Conclído - 2019-05-27 14:36:20.086357 ...\n"
     ]
    }
   ],
   "source": [
    "print('Processo Iniciado - %s ...\\n' %str(datetime.now()))\n",
    "\n",
    "# Chamada das funções criadas acima, passando os parametros esperados:\n",
    "df = extrair_dados_arquivo()\n",
    "df = modifica_dados_dataframe(df)\n",
    "lista = analisar_dados_dataframe(df)\n",
    "grava_dados_csv(lista)\n",
    "\n",
    "print('Processo Conclído - %s ...' %str(datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
